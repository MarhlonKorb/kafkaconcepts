# How to Run:

1. Ensure Docker Desktop is installed on your computer.
2. Download the project directory and navigate to src/main/docker.
3. Run the command docker kafka -d to start the application containers.
4. Open your IDE and configure the project to run with Java 17.
5. Run the project.
6. Publish a topic using the Kafka UI interface by accessing it locally at http://localhost:8080/, selecting "topics," and clicking "Produce Message."

## When to Use Kafka?

Real-Time Data Stream Processing (Streaming Analytics)
Kafka is ideal for collecting and processing data in real time, such as monitoring user activities on websites, financial transactions, or IoT sensor events.
Examples: clickstream processing on websites, real-time fraud detection, and metric analysis in dashboards.

Data Integration Between Systems
Kafka facilitates integration between heterogeneous systems by transferring data between databases, storage systems, and applications.
Kafka Connect is a specific Kafka API that enables reliable data integration, allowing data to be synchronized continuously between different sources (e.g., MySQL to MongoDB).

Data Ingestion for Data Lakes and Data Warehouses
Kafka is often used for data ingestion into data lakes and data warehouses, such as Amazon S3, Google BigQuery, or Snowflake, allowing data from multiple sources to be collected and stored in a centralized structure.
Companies use Kafka to ensure large volumes of data are processed and available for later analysis and reporting.

Event Sourcing
In event-driven systems, Kafka is used to capture each change as an event, storing a detailed audit trail of everything that happened in the system.
This approach is useful for reconstructing states or replaying events in the event of failures, especially in banking and financial systems.

Microservices Communication
Kafka is widely used for asynchronous communication between microservices, helping to manage and scale the exchange of information between services.
Acting as a middleware layer, it decouples communication between services, facilitating system scalability and resilience.

Monitoring and Log Aggregation
Kafka can act as a central hub for collecting application logs and metrics, used in real-time monitoring and log analysis systems.
It is commonly integrated with tools such as the ELK Stack (Elasticsearch, Logstash, Kibana) and Grafana for log visualization and analysis.

IoT and Telemetry Applications
In Internet of Things (IoT) systems, Kafka is used to process large amounts of data generated by sensors and devices in real time.
Telemetry applications, such as autonomous vehicle systems, smart agriculture, and connected cities, use Kafka to process data with minimal latency.

Batch Processing and ETL (Extract, Transform, Load)
Kafka is used for batch processing pipelines and ETL, capturing data from different systems, performing real-time transformations, and loading it into target systems.
Combined with tools like Apache Spark, Kafka enables real-time ETL for large-scale data analysis.

Real-Time Machine Learning Support
For machine learning systems that require continuous data input for training and inference, Kafka provides up-to-date data for models.
Kafka enables integration with ML platforms, allowing new data to be continuously processed and used to update predictions or train models in real time.

## How it works?

![Image Description](https://media.licdn.com/dms/image/v2/D5612AQGr8IpsMekKfQ/article-cover_image-shrink_600_2000/article-cover_image-shrink_600_2000/0/1687242848800?e=1736985600&v=beta&t=jYom1yCil6k3JcRCdt7hW8vn50_jwQTHvQzb1RTWTLw)

image credits: https://www.linkedin.com/pulse/unlock-kafkas-full-potential-intro-best-devops-priyal-walpita/
